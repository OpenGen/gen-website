<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS --> 
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" integrity="sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z" crossorigin="anonymous">
    <link rel="stylesheet" href="/assets/css/styles.css">

    <!-- MathJax -->
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <title>Introduction to Modeling in Gen</title>
  </head>
  <body>
    
    <header class="navbar navbar-expand navbar-dark flex-column flex-md-row bd-navbar">
  <a class="navbar-brand mr-0 mr-md-2" href="/" aria-label="Gen">
<svg version="1.1" width="36" height="36" viewBox="0.0 0.0 433.7244094488189 432.76640419947506" fill="none" stroke="none" stroke-linecap="square" stroke-miterlimit="10" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg"><clipPath id="p.0"><path d="m0 0l433.7244 0l0 432.76642l-433.7244 0l0 -432.76642z" clip-rule="nonzero"/></clipPath><g clip-path="url(#p.0)"><path fill="#000000" fill-opacity="0.0" d="m0 0l433.7244 0l0 432.76642l-433.7244 0z" fill-rule="evenodd"/><path fill="#000000" fill-opacity="0.0" d="m526.3617 215.97453l0 0c0 -112.37038 91.09418 -203.46457 203.4646 -203.46457l0 0c53.96216 0 105.71411 21.436382 143.87115 59.593395c38.156982 38.157005 59.593384 89.90901 59.593384 143.87117l0 0c0 112.37038 -91.09418 203.46455 -203.46454 203.46455l0 0c-112.37042 0 -203.4646 -91.09418 -203.4646 -203.46455z" fill-rule="evenodd"/><path stroke="#ffffff" stroke-width="16.0" stroke-linejoin="round" stroke-linecap="butt" d="m526.3617 215.97453l0 0c0 -112.37038 91.09418 -203.46457 203.4646 -203.46457l0 0c53.96216 0 105.71411 21.436382 143.87115 59.593395c38.156982 38.157005 59.593384 89.90901 59.593384 143.87117l0 0c0 112.37038 -91.09418 203.46455 -203.46454 203.46455l0 0c-112.37042 0 -203.4646 -91.09418 -203.4646 -203.46455z" fill-rule="evenodd"/><path fill="#000000" fill-opacity="0.0" d="m95.40751 8.810548l290.11023 0l0 288.18896l-290.11023 0z" fill-rule="evenodd"/><path fill="#ffffff" d="m308.04813 300.89618q-12.859375 15.421875 -36.375 23.921875q-23.5 8.484375 -52.09375 8.484375q-30.03125 0 -52.671875 -13.09375q-22.625 -13.109375 -34.9375 -38.046875q-12.312492 -24.9375 -12.624992 -58.625l0 -15.71875q0 -34.640625 11.671867 -59.96875q11.671875 -25.34375 33.671875 -38.765625q22.0 -13.421875 51.546875 -13.421875q41.140625 0 64.328125 19.625q23.203125 19.609375 27.484375 57.109375l-46.375 0q-3.171875 -19.859375 -14.0625 -29.0625q-10.875 -9.21875 -29.9375 -9.21875q-24.3125 0 -37.015625 18.265625q-12.703125 18.265625 -12.875 54.328125l0 14.765625q0 36.375 13.8125 54.96875q13.828125 18.578125 40.515625 18.578125q26.859375 0 38.296875 -11.4375l0 -39.875l-43.375 0l0 -35.09375l91.015625 0l0 92.28125z" fill-rule="nonzero"/><path fill="#000000" fill-opacity="0.0" d="m20.661194 84.271866l0 0c0 -36.022438 29.201962 -65.224396 65.2244 -65.224396l260.8898 0l0 0c17.298584 0 33.88864 6.8718376 46.120605 19.103786c12.231934 12.231945 19.10379 28.82203 19.10379 46.120613l0 260.88977c0 36.02243 -29.201965 65.224396 -65.224396 65.224396l-260.8898 0c-36.02244 0 -65.2244 -29.201965 -65.2244 -65.224396z" fill-rule="evenodd"/><path stroke="#ffffff" stroke-width="24.0" stroke-linejoin="round" stroke-linecap="butt" d="m20.661194 84.271866l0 0c0 -36.022438 29.201962 -65.224396 65.2244 -65.224396l260.8898 0l0 0c17.298584 0 33.88864 6.8718376 46.120605 19.103786c12.231934 12.231945 19.10379 28.82203 19.10379 46.120613l0 260.88977c0 36.02243 -29.201965 65.224396 -65.224396 65.224396l-260.8898 0c-36.02244 0 -65.2244 -29.201965 -65.2244 -65.224396z" fill-rule="evenodd"/></g></svg>
</a>
  <div class="navbar-nav-scroll">
    <ul class="navbar-nav bd-navbar-nav flex-row">
    
      <li class="nav-item">
        <a class="nav-link " href="/">Home</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-link " href="https://www.gen.dev/dev/">Documentation</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-link " href="/tutorials/">Tutorials</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-link " href="https://github.com/probcomp/Gen.jl">Source</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-link " href="/ecosystem/">Ecosystem</a>
      </li>
    
    </ul>
  </div>

</header>



<main role="main">
    <br/>
<div class="container">
<h1 id="tutorial-introduction-to-modeling-in-gen">Tutorial: Introduction to Modeling in Gen</h1>

<p>Welcome! In this tutorial, you’ll get your feet wet with Gen, a
multi-paradigm platform for probabilistic modeling and inference. By
“multi-paradigm,” we mean that Gen supports many different approaches to
modeling and inference:</p>

<ul>
  <li>
    <p>Unsupervised learning and posterior inference in generative models using
Monte Carlo,  variational, EM, and stochastic gradient techniques.</p>
  </li>
  <li>
    <p>Supervised learning of conditional inference models (e.g. supervised
classification and regression).</p>
  </li>
  <li>
    <p>Hybrid approaches including amortized inference / inference compilation,
variational autoencoders, and semi-supervised learning.</p>
  </li>
</ul>

<p>Don’t worry if you haven’t seen some of these approaches before. One goal of
these tutorials will be to introduce you to a subset of them,
from a unified probabilistic programming perspective.</p>

<h4 id="in-this-tutorial">In this Tutorial</h4>

<p>Approaching a problem from a probabilistic perspective requires both <em>modeling</em>
and <em>inference</em>:</p>

<ul>
  <li>
    <p><strong>Modeling</strong>: You first need to frame the problem — and any assumptions you
bring to the table — as a probabilistic model. A huge variety of problems can
be viewed from a modeling &amp; inference lens, if you set them up properly. 
<strong>This notebook is about how to think of problems in this light, and how to use Gen</strong>
<strong>to formally specify your assumptions and the tasks you wish to solve.</strong></p>
  </li>
  <li>
    <p><strong>Inference</strong>: You then need to do the hard part: inference, that is, 
solving the problem. In this notebook, we’ll use a particularly simple 
<em>generic</em> inference algorithm: importance sampling with the prior as our
proposal distributions. With enough computation, the algorithm 
can in theory solve any modeling and inference problem, but in practice, for most problems of
interest, it is too slow to achieve accurate results in a reasonable amount of time. 
<strong>Future tutorials introduce some of Gen’s</strong>
<strong>programmable inference features</strong>, 
which let you tailor the inference algorithm for use with more complex models (Gen will still automate the math!).</p>
  </li>
</ul>

<p>Throughout this 
tutorial,
we will emphasize key degrees of modeling flexibility
afforded by the probabilistic programming approach, for example:</p>

<ul>
  <li>
    <p>Using a stochastic branching and function abstraction to express
uncertainty about which of multiple models is appropriate.</p>
  </li>
  <li>
    <p>Representing models with an unbounded number of parameters (a ‘Bayesian
non-parametric’ model) using loops and recursion.</p>
  </li>
</ul>

<p>We’ll also introduce a technique for validating a model and
inference algorithm by predicting new data from inferred parameters, and
comparing this data to the observed data set.</p>

<p>However, this
tutorial
does not exhaustively cover all features of Gen’s modeling language. 
For example, Gen’s modeling combinators and its static modeling language
enable improved performance, but are not covered here.</p>

<h2 id="outline">Outline</h2>

<p><strong>Section 1.</strong> <a href="#julia-gen-jupyter">Julia, Gen, and this Jupyter notebook</a></p>

<p><strong>Section 2.</strong> <a href="#writing-model">Writing a probabilistic model as a generative function</a></p>

<p><strong>Section 3.</strong> <a href="#doing-inference">Doing posterior inference</a></p>

<p><strong>Section 4.</strong> <a href="#predicting-data">Predicting new data</a></p>

<p><strong>Section 5.</strong> <a href="#calling-functions">Calling other generative functions</a></p>

<p><strong>Section 6.</strong> <a href="#infinite-space">Modeling with an unbounded number of parameters</a></p>

<h2 id="1-julia-gen-and-this-jupyter-notebook--">1. Julia, Gen, and this Jupyter notebook  <a name="julia-gen-jupyter"></a></h2>

<p>Gen is a package for the Julia language. The package can be loaded with:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">using</span> <span class="n">Gen</span>
</code></pre></div></div>

<p>Gen programs typically consist of a combination of (i) probabilistic models written in modeling languages and (ii) inference programs written in regular Julia code. Gen provides a built-in modeling language that is itself based on Julia.</p>

<p>This tutorial uses a Jupyter notebook. All cells in the notebook are regular Julia cells. In Julia, semicolons are optional at the end of statements; we will use them at the end of some cells so that the value of the cell is not printed.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="x">;</span>
</code></pre></div></div>

<p>This notebook uses the <a href="https://github.com/JuliaPlots/Plots.jl">Plots.jl</a> Julia package for plotting.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">using</span> <span class="n">Plots</span>
</code></pre></div></div>

<p>This notebook will make use of Julia symbols. Note that a Julia symbol is different from a Julia string:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">typeof</span><span class="x">(:</span><span class="n">foo</span><span class="x">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Symbol
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">typeof</span><span class="x">(</span><span class="s">"foo"</span><span class="x">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>String
</code></pre></div></div>

<h2 id="2-writing-a-probabilistic-model-as-a-generative-function--">2. Writing a probabilistic model as a generative function  <a name="writing-model"></a></h2>

<p>Probabilistic models are represented in Gen as <em>generative functions</em>.
Generative functions are used to represent a variety of different types of
probabilistic computations including generative models, inference models,
custom proposal distributions, and variational approximations (see the <a href="https://probcomp.github.io/Gen/dev/ref/gfi/">Gen
documentation</a> or the 
<a href="https://dl.acm.org/doi/10.1145/3314221.3314642">paper</a>). In this
tutorial,
we focus on implementing <em>generative models</em>. A generative model represents
a data-generating process; as such, it encodes any assumptions we have about
our data and our problem domain.</p>

<p>The simplest way to construct a generative function is by using the <a href="https://probcomp.github.io/Gen/dev/ref/modeling/">built-in
modeling DSL</a>. Generative
functions written in the built-in modeling DSL are based on Julia function
definition syntax, but are prefixed with the <code class="highlighter-rouge">@gen</code> macro:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@gen</span> <span class="k">function</span><span class="nf"> function_name_here</span><span class="x">(</span><span class="n">input_arguments</span><span class="x">)</span>
    <span class="c"># Function body...</span>
<span class="k">end</span>
</code></pre></div></div>

<p>The function represents the data-generating process we are modeling.
Conceptually, every time we run the function, it should generate a new
“synthetic dataset” in line with our assumptions. Along the way, it will make
random choices; each random choice it makes can be thought of as adding
a random variable to a probabilistic model.</p>

<p>Within the function body, most Julia code is permitted, but random choices use
special syntax that annotates them with an <em>address</em>:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="x">{</span><span class="n">addr</span><span class="x">}</span> <span class="o">~</span> <span class="n">distribution</span><span class="x">(</span><span class="n">parameters</span><span class="x">)</span>
</code></pre></div></div>

<p>A simple example of such an invocation is a normal distribution parametrized
with mean 0 and standard deviation 1:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_variable</span> <span class="o">=</span> <span class="x">{:</span><span class="n">my_variable_address</span><span class="x">}</span> <span class="o">~</span> <span class="n">normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>
</code></pre></div></div>

<p>Every random choice must be given an <em>address</em>, which can be
an arbitrary value—but we often use a symbol. 
(<code class="highlighter-rouge">:my_variable_address</code> is a symbol in the Julia language.)
Think of the address as the name of a particular random choice, which
is distinct from the name of the variable. For example, consider
the following code:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="x">{:</span><span class="n">initial_x</span><span class="x">}</span> <span class="o">~</span> <span class="n">normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>
<span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="mi">0</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="x">({:</span><span class="n">addition_to_x</span><span class="x">}</span> <span class="o">~</span> <span class="n">normal</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span> <span class="mi">1</span><span class="x">))</span>
<span class="k">end</span>
</code></pre></div></div>

<p>This code manipulates a single variable, <code class="highlighter-rouge">x</code>, but may make up to two random
choices: <code class="highlighter-rouge">:initial_x</code> and <code class="highlighter-rouge">:addition_to_x</code>.</p>

<p>Note that we can only use <code class="highlighter-rouge">~</code> to give addresses to <em>random choices</em>. 
The following will <em>not</em> work because the code is trying to trace the
expression <code class="highlighter-rouge">sin(x)</code> which is an invocation of an ordinary Julia function, not
a distribution.</p>
<pre><code class="language-Julia"># INVALID:
my_variable = {:not_a_random_choice} ~ sin(x)
</code></pre>
<p>(We will see a bit later that it is <em>also</em> possible to use <code class="highlighter-rouge">~</code>
to sample from helper <em>generative functions</em>, not just primitive 
distributions like <code class="highlighter-rouge">normal</code>. But for now, think of <code class="highlighter-rouge">~</code> as being
for making random choices.)</p>

<h3 id="example-bayesian-linear-regression">Example: Bayesian linear regression</h3>

<p>Suppose we have a dataset of points $(x, y)$ in the plane, and we’d like 
to infer a likely slope and intercept that explains their (linear) relationship.
To approach this problem from a probabilistic perspective, we first need to
develop a model. The model answers the question: how might this dataset have
come to be? It also encodes our assumptions, e.g., our assumption that our
data is explained by a linear relationship between $x$ and $y$.</p>

<p>The generative function below represents a probabilistic model of a linear
relationship in the x-y plane. Given a set of $x$ coordinates, it randomly
chooses a line in the plane and generates corresponding $y$ coordinates so
that each $(x, y)$ is near the line. We might think of this function as
modeling house prices as a function of square footage, or the measured volume
of a gas as a function of its measured temperature.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@gen</span> <span class="k">function</span><span class="nf"> line_model</span><span class="x">(</span><span class="n">xs</span><span class="o">::</span><span class="n">Vector</span><span class="x">{</span><span class="kt">Float64</span><span class="x">})</span>
    <span class="c"># We begin by sampling a slope and intercept for the line.</span>
    <span class="c"># Before we have seen the data, we don't know the values of</span>
    <span class="c"># these parameters, so we treat them as random choices. The</span>
    <span class="c"># distributions they are drawn from represent our prior beliefs</span>
    <span class="c"># about the parameters: in this case, that neither the slope nor the</span>
    <span class="c"># intercept will be more than a couple points away from 0.</span>
    <span class="n">slope</span> <span class="o">=</span> <span class="x">({:</span><span class="n">slope</span><span class="x">}</span> <span class="o">~</span> <span class="n">normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="mi">1</span><span class="x">))</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="x">({:</span><span class="n">intercept</span><span class="x">}</span> <span class="o">~</span> <span class="n">normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="mi">2</span><span class="x">))</span>
    
    <span class="c"># We define a function to compute y for a given x</span>
    <span class="k">function</span><span class="nf"> y</span><span class="x">(</span><span class="n">x</span><span class="x">)</span>
        <span class="k">return</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">intercept</span>
    <span class="k">end</span>

    <span class="c"># Given the slope and intercept, we can sample y coordinates</span>
    <span class="c"># for each of the x coordinates in our input vector.</span>
    <span class="k">for</span> <span class="x">(</span><span class="n">i</span><span class="x">,</span> <span class="n">x</span><span class="x">)</span> <span class="k">in</span> <span class="n">enumerate</span><span class="x">(</span><span class="n">xs</span><span class="x">)</span>
        <span class="c"># Note that we name each random choice in this loop</span>
        <span class="c"># slightly differently: the first time through,</span>
        <span class="c"># the name (:y, 1) will be used, then (:y, 2) for</span>
        <span class="c"># the second point, and so on.</span>
        <span class="x">({(:</span><span class="n">y</span><span class="x">,</span> <span class="n">i</span><span class="x">)}</span> <span class="o">~</span> <span class="n">normal</span><span class="x">(</span><span class="n">y</span><span class="x">(</span><span class="n">x</span><span class="x">),</span> <span class="mf">0.1</span><span class="x">))</span>
    <span class="k">end</span>

    <span class="c"># Most of the time, we don't care about the return</span>
    <span class="c"># value of a model, only the random choices it makes.</span>
    <span class="c"># It can sometimems be useful to return something</span>
    <span class="c"># meaningful, however; here, we return the function `y`.</span>
    <span class="k">return</span> <span class="n">y</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<p>The generative function takes as an argument a vector of x-coordinates. We create one below:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xs</span> <span class="o">=</span> <span class="x">[</span><span class="o">-</span><span class="mf">5.</span><span class="x">,</span> <span class="o">-</span><span class="mf">4.</span><span class="x">,</span> <span class="o">-</span><span class="mf">3.</span><span class="x">,</span> <span class="o">-</span><span class="mf">2.</span><span class="x">,</span> <span class="o">-</span><span class="mf">1.</span><span class="x">,</span> <span class="mf">0.</span><span class="x">,</span> <span class="mf">1.</span><span class="x">,</span> <span class="mf">2.</span><span class="x">,</span> <span class="mf">3.</span><span class="x">,</span> <span class="mf">4.</span><span class="x">,</span> <span class="mf">5.</span><span class="x">];</span>
</code></pre></div></div>

<p>Given this vector, the generative function samples a random choice
representing the slope of a line from a normal distribution with mean 0 and
standard deviation 1, and a random choice representing the intercept of a
line from a normal distribution with mean 0 and standard deviation 2. In
Bayesian statistics terms, these distributions are the <em>prior distributions</em>
of the slope and intercept respectively. Then, the function samples values
for the y-coordinates corresponding to each of the provided x-coordinates.</p>

<p>This generative function returns a function <code class="highlighter-rouge">y</code> encoding the slope
and intercept. 
We can run the model like we run a regular Julia function:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">line_model</span><span class="x">(</span><span class="n">xs</span><span class="x">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>y (generic function with 1 method)
</code></pre></div></div>

<p>This gives us the return value of the model, but we
may be more interested in <em>the values of the random choices</em> that
<code class="highlighter-rouge">line_model</code> makes. <strong>Crucially, each random choice is annotated with a
unique <em>address</em>.</strong> A random choice is assigned an address using the <code class="highlighter-rouge">{addr} ~ ...</code>
keyword. Addresses can be any Julia value. In this program, there are two
types of addresses used – Julia symbols and tuples of symbols and integers.
Note that within the <code class="highlighter-rouge">for</code> loop, the same line of code is executed multiple
times, but each time, the random choice it makes is given a distinct address.</p>

<p>Although the random choices are not included in the return value, they <em>are</em>
included in the <em>execution trace</em> of the generative function. We can run the
generative function and obtain its trace using the <a href="https://probcomp.github.io/Gen/dev/ref/gfi/#Gen.simulate"><code class="highlighter-rouge">
simulate</code></a> method
from the Gen API:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trace</span> <span class="o">=</span> <span class="n">Gen</span><span class="o">.</span><span class="n">simulate</span><span class="x">(</span><span class="n">line_model</span><span class="x">,</span> <span class="x">(</span><span class="n">xs</span><span class="x">,));</span>
</code></pre></div></div>

<p>This method takes the function to be executed, and a tuple of arguments to the function, and returns a trace and a second value that we will not be using in this tutorial. When we print the trace, we see that it is a complex data structure.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">println</span><span class="x">(</span><span class="n">trace</span><span class="x">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Gen.DynamicDSLTrace{DynamicDSLFunction{Any}}(DynamicDSLFunction{Any}(Dict{Symbol, Any}(), Dict{Symbol, Any}(), Type[Vector{Float64}], false, Union{Nothing, Some{Any}}[nothing], var"##line_model#274", Bool[0], false), Trie{Any, Gen.ChoiceOrCallRecord}(Dict{Any, Gen.ChoiceOrCallRecord}((:y, 11) =&gt; Gen.ChoiceOrCallRecord{Float64}(9.383490918197065, -0.044423239264899994, NaN, true), (:y, 3) =&gt; Gen.ChoiceOrCallRecord{Float64}(-2.8292929611830555, 1.3417576659518766, NaN, true), :intercept =&gt; Gen.ChoiceOrCallRecord{Float64}(1.8319666784347306, -2.0315984526265156, NaN, true), (:y, 1) =&gt; Gen.ChoiceOrCallRecord{Float64}(-5.959713829773403, 1.1304941689886963, NaN, true), (:y, 8) =&gt; Gen.ChoiceOrCallRecord{Float64}(4.976431986524593, 1.225414500885959, NaN, true), (:y, 7) =&gt; Gen.ChoiceOrCallRecord{Float64}(3.2821949214637147, 0.9430035125891707, NaN, true), (:y, 5) =&gt; Gen.ChoiceOrCallRecord{Float64}(0.2548748434761596, 1.3292402671095034, NaN, true), (:y, 6) =&gt; Gen.ChoiceOrCallRecord{Float64}(1.9173535451258832, 1.0191007096227396, NaN, true), (:y, 10) =&gt; Gen.ChoiceOrCallRecord{Float64}(7.954210096018949, 1.236889673045222, NaN, true), :slope =&gt; Gen.ChoiceOrCallRecord{Float64}(1.5441050822599893, -2.111068785735237, NaN, true), (:y, 4) =&gt; Gen.ChoiceOrCallRecord{Float64}(-1.1305752102234436, 0.5940207818854444, NaN, true), (:y, 2) =&gt; Gen.ChoiceOrCallRecord{Float64}(-4.346401246132024, 1.3834569033725725, NaN, true), (:y, 9) =&gt; Gen.ChoiceOrCallRecord{Float64}(6.53337380525437, 1.1449621654185604, NaN, true)), Dict{Any, Trie{Any, Gen.ChoiceOrCallRecord}}()), false, 7.161249871243092, 0.0, ([-5.0, -4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0],), var"#y#1"{Float64, Float64}(1.8319666784347306, 1.5441050822599893))
</code></pre></div></div>

<p>A trace of a generative function contains various information about an execution of the function. For example, it contains the arguments on which the function was run, which are available with the API method <code class="highlighter-rouge">get_args</code>:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Gen</span><span class="o">.</span><span class="n">get_args</span><span class="x">(</span><span class="n">trace</span><span class="x">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>([-5.0, -4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0],)
</code></pre></div></div>

<p>The trace also contains the value of the random choices, stored in a map from address to value called a <em>choice map</em>. This map is available through the API method <a href=""><code class="highlighter-rouge">get_choices</code></a>:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Gen</span><span class="o">.</span><span class="n">get_choices</span><span class="x">(</span><span class="n">trace</span><span class="x">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>│
├── (:y, 11) : 9.383490918197065
│
├── (:y, 3) : -2.8292929611830555
│
├── :intercept : 1.8319666784347306
│
├── (:y, 1) : -5.959713829773403
│
├── (:y, 8) : 4.976431986524593
│
├── (:y, 7) : 3.2821949214637147
│
├── (:y, 5) : 0.2548748434761596
│
├── (:y, 6) : 1.9173535451258832
│
├── (:y, 10) : 7.954210096018949
│
├── :slope : 1.5441050822599893
│
├── (:y, 4) : -1.1305752102234436
│
├── (:y, 2) : -4.346401246132024
│
└── (:y, 9) : 6.53337380525437
</code></pre></div></div>

<p>We can pull out individual values from this map using Julia’s subscripting syntax <code class="highlighter-rouge">[...]</code>:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">choices</span> <span class="o">=</span> <span class="n">Gen</span><span class="o">.</span><span class="n">get_choices</span><span class="x">(</span><span class="n">trace</span><span class="x">)</span>
<span class="n">choices</span><span class="x">[:</span><span class="n">slope</span><span class="x">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.5441050822599893
</code></pre></div></div>

<p>We can also read the value of a random choice directly from the trace, without having to use <code class="highlighter-rouge">get_choices</code> first:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trace</span><span class="x">[:</span><span class="n">slope</span><span class="x">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.5441050822599893
</code></pre></div></div>

<p>The return value is also recorded in the trace, and is accessible with the <code class="highlighter-rouge">get_retval</code> API method:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Gen</span><span class="o">.</span><span class="n">get_retval</span><span class="x">(</span><span class="n">trace</span><span class="x">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>y (generic function with 1 method)
</code></pre></div></div>

<p>Or we can access the return value directly from the trace via the syntactic sugar <code class="highlighter-rouge">trace[]</code>:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trace</span><span class="x">[]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>y (generic function with 1 method)
</code></pre></div></div>

<p>In order to understand the probabilistic behavior of a generative function, it is helpful to be able to visualize its traces. Below, we define a function that uses PyPlot to render a trace of the generative function above. The rendering shows the x-y data points and the line that is represented by the slope and intercept choices.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span><span class="nf"> render_trace</span><span class="x">(</span><span class="n">trace</span><span class="x">;</span> <span class="n">show_data</span><span class="o">=</span><span class="n">true</span><span class="x">)</span>
    
    <span class="c"># Pull out xs from the trace</span>
    <span class="n">xs</span><span class="x">,</span> <span class="o">=</span> <span class="n">get_args</span><span class="x">(</span><span class="n">trace</span><span class="x">)</span>
    
    <span class="n">xmin</span> <span class="o">=</span> <span class="n">minimum</span><span class="x">(</span><span class="n">xs</span><span class="x">)</span>
    <span class="n">xmax</span> <span class="o">=</span> <span class="n">maximum</span><span class="x">(</span><span class="n">xs</span><span class="x">)</span>

    <span class="c"># Pull out the return value, useful for plotting</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">get_retval</span><span class="x">(</span><span class="n">trace</span><span class="x">)</span>
    
    <span class="c"># Draw the line</span>
    <span class="n">test_xs</span> <span class="o">=</span> <span class="n">collect</span><span class="x">(</span><span class="n">range</span><span class="x">(</span><span class="o">-</span><span class="mi">5</span><span class="x">,</span> <span class="n">stop</span><span class="o">=</span><span class="mi">5</span><span class="x">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">1000</span><span class="x">))</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plot</span><span class="x">(</span><span class="n">test_xs</span><span class="x">,</span> <span class="n">map</span><span class="x">(</span><span class="n">y</span><span class="x">,</span> <span class="n">test_xs</span><span class="x">),</span> <span class="n">color</span><span class="o">=</span><span class="s">"black"</span><span class="x">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="x">,</span> <span class="n">label</span><span class="o">=</span><span class="n">nothing</span><span class="x">,</span>
                <span class="n">xlim</span><span class="o">=</span><span class="x">(</span><span class="n">xmin</span><span class="x">,</span> <span class="n">xmax</span><span class="x">),</span> <span class="n">ylim</span><span class="o">=</span><span class="x">(</span><span class="n">xmin</span><span class="x">,</span> <span class="n">xmax</span><span class="x">))</span>

    <span class="k">if</span> <span class="n">show_data</span>
        <span class="n">ys</span> <span class="o">=</span> <span class="x">[</span><span class="n">trace</span><span class="x">[(:</span><span class="n">y</span><span class="x">,</span> <span class="n">i</span><span class="x">)]</span> <span class="k">for</span> <span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="x">:</span><span class="n">length</span><span class="x">(</span><span class="n">xs</span><span class="x">)]</span>
        
        <span class="c"># Plot the data set</span>
        <span class="n">scatter!</span><span class="x">(</span><span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="n">c</span><span class="o">=</span><span class="s">"black"</span><span class="x">,</span> <span class="n">label</span><span class="o">=</span><span class="n">nothing</span><span class="x">)</span>
    <span class="k">end</span>
    
    <span class="k">return</span> <span class="n">fig</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">render_trace</span><span class="x">(</span><span class="n">trace</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_39_0.svg" alt="svg" /></p>

<p>Because a generative function is stochastic, we need to visualize many runs in order to understand its behavior. The cell below renders a grid of traces.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span><span class="nf"> grid</span><span class="x">(</span><span class="n">renderer</span><span class="o">::</span><span class="n">Function</span><span class="x">,</span> <span class="n">traces</span><span class="x">)</span>
    <span class="n">Plots</span><span class="o">.</span><span class="n">plot</span><span class="x">(</span><span class="n">map</span><span class="x">(</span><span class="n">renderer</span><span class="x">,</span> <span class="n">traces</span><span class="x">)</span><span class="o">...</span><span class="x">)</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">traces</span> <span class="o">=</span> <span class="x">[</span><span class="n">Gen</span><span class="o">.</span><span class="n">simulate</span><span class="x">(</span><span class="n">line_model</span><span class="x">,</span> <span class="x">(</span><span class="n">xs</span><span class="x">,))</span> <span class="k">for</span> <span class="n">_</span><span class="o">=</span><span class="mi">1</span><span class="x">:</span><span class="mi">12</span><span class="x">]</span>
<span class="n">grid</span><span class="x">(</span><span class="n">render_trace</span><span class="x">,</span> <span class="n">traces</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_42_0.svg" alt="svg" /></p>

<hr />
<h3 id="exercise">Exercise</h3>

<p>Write a generative function that uses the same address twice. Run it to see what happens.</p>

<hr />
<h3 id="exercise-1">Exercise</h3>

<p>Write a model that generates a sine wave with random phase, period and
amplitude, and then generates y-coordinates from a given vector of
x-coordinates by adding noise to the value of the wave at each x-coordinate.
Use a  <code class="highlighter-rouge">gamma(1, 1)</code> prior distribution for the period, and a <code class="highlighter-rouge">gamma(1, 1)</code>
prior distribution on the amplitude (see
<a href="https://probcomp.github.io/Gen/dev/ref/distributions/#Gen.gamma"><code class="highlighter-rouge">Gen.gamma</code></a>).
Sampling from a Gamma distribution will ensure to give us postive real values.
Use a uniform distribution between 0 and $2\pi$ for the phase (see
<a href="https://probcomp.github.io/Gen/dev/ref/distributions/#Gen.uniform"><code class="highlighter-rouge">Gen.uniform</code></a>).</p>

<p>The sine wave should implement:</p>

<p>$ y(x) = a \sin(2\pi \frac{1}{p} x + \varphi)$,</p>

<p>where $a$ is the amplitude, $p$ is the period and $\varphi$ is the phase.  In
Julia the constant $\pi$ can be expressed as either <code class="highlighter-rouge">pi</code> or <code class="highlighter-rouge">π</code> (unicode).</p>

<p>When calling <code class="highlighter-rouge">trace = Gen.simulate(sine_model, (xs,))</code>, the following choices should appear:</p>
<ul>
  <li>amplitude: <code class="highlighter-rouge">trace[:amplitude]</code></li>
  <li>period: <code class="highlighter-rouge">trace[:period]</code></li>
  <li>phase: <code class="highlighter-rouge">trace[:phase]</code></li>
</ul>

<p>We have provided some starter code for the sine wave model:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@gen</span> <span class="k">function</span><span class="nf"> sine_model</span><span class="x">(</span><span class="n">xs</span><span class="o">::</span><span class="n">Vector</span><span class="x">{</span><span class="kt">Float64</span><span class="x">})</span>
    
    <span class="c"># &lt; your code here, for sampling a phase, period, and amplitude &gt;</span>

    <span class="k">function</span><span class="nf"> y</span><span class="x">(</span><span class="n">x</span><span class="x">)</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="c"># &lt; Edit this line to compute y for a given x &gt;</span>
    <span class="k">end</span>
    
    <span class="k">for</span> <span class="x">(</span><span class="n">i</span><span class="x">,</span> <span class="n">x</span><span class="x">)</span> <span class="k">in</span> <span class="n">enumerate</span><span class="x">(</span><span class="n">xs</span><span class="x">)</span>
        <span class="x">{(:</span><span class="n">y</span><span class="x">,</span> <span class="n">i</span><span class="x">)}</span> <span class="o">~</span> <span class="n">normal</span><span class="x">(</span><span class="n">y</span><span class="x">(</span><span class="n">x</span><span class="x">),</span> <span class="mf">0.1</span><span class="x">)</span>
    <span class="k">end</span>
    
    <span class="k">return</span> <span class="n">y</span> <span class="c"># We return the y function so it can be used for plotting, below. </span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">traces</span> <span class="o">=</span> <span class="x">[</span><span class="n">Gen</span><span class="o">.</span><span class="n">simulate</span><span class="x">(</span><span class="n">sine_model</span><span class="x">,</span> <span class="x">(</span><span class="n">xs</span><span class="x">,))</span> <span class="k">for</span> <span class="n">_</span><span class="o">=</span><span class="mi">1</span><span class="x">:</span><span class="mi">12</span><span class="x">];</span>
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grid</span><span class="x">(</span><span class="n">render_trace</span><span class="x">,</span> <span class="n">traces</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_49_0.svg" alt="svg" /></p>

<hr />

<!-- # Solution:
@gen function sine_model(xs)
    period = {:period} ~ gamma(1, 1)
    amplitude = {:amplitude} ~ gamma(1, 1)
    phase = {:phase} ~ uniform(0, 2*pi)

    # Define a deterministic sine wave with the values above
    function y(x)
        return amplitude * sin(x * (2 * pi / period) + phase)
    end

    for (i, x) in enumerate(xs)
        {(:y, i)} ~ normal(y(x), 0.1)
    end

    return y
end; -->

<h2 id="3-doing-posterior-inference--">3. Doing Posterior inference  <a name="doing-inference"></a></h2>

<p>Of course, we don’t really care about generating lots of pictures of lines
(or sine waves). We’d really like to begin with an actual dataset of observed
$(x, y)$ points, and infer the corresponding slope and intercept (or phase,
period, and amplitude). This task is called <em>posterior inference</em>.</p>

<p>We now will provide a data set of y-coordinates and try to draw inferences
about the process that generated the data. We begin with the following data
set:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ys</span> <span class="o">=</span> <span class="x">[</span><span class="mf">6.75003</span><span class="x">,</span> <span class="mf">6.1568</span><span class="x">,</span> <span class="mf">4.26414</span><span class="x">,</span> <span class="mf">1.84894</span><span class="x">,</span> <span class="mf">3.09686</span><span class="x">,</span> <span class="mf">1.94026</span><span class="x">,</span> <span class="mf">1.36411</span><span class="x">,</span> <span class="o">-</span><span class="mf">0.83959</span><span class="x">,</span> <span class="o">-</span><span class="mf">0.976</span><span class="x">,</span> <span class="o">-</span><span class="mf">1.93363</span><span class="x">,</span> <span class="o">-</span><span class="mf">2.91303</span><span class="x">];</span>
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">scatter</span><span class="x">(</span><span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"black"</span><span class="x">,</span> <span class="n">label</span><span class="o">=</span><span class="n">nothing</span><span class="x">,</span> <span class="n">title</span><span class="o">=</span><span class="s">"Observed data (linear)"</span><span class="x">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s">"X"</span><span class="x">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s">"Y"</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_53_0.svg" alt="svg" /></p>

<p>We will assume that the line model was responsible for generating the data,
and infer values of the slope and intercept that explain the data.</p>

<p>To do this, we write a simple <em>inference program</em> that takes the model we are
assuming generated our data, the data set, and the amount of computation to
perform, and returns a trace of the function that is approximately sampled
from the <em>posterior distribution</em> on traces of the function, given the
observed data. That is, the inference program will try to find a trace that
well explains the dataset we created above. We can inspect that trace to find
estimates of the slope and intercept of a line that fits the data.</p>

<p>Functions like <code class="highlighter-rouge">importance_resampling</code> expect us to provide a <em>model</em> and
also an <em>choice map</em> representing our data set and relating it to the model.
A choice map maps random choice addresses from the model to values from our
data set. Here, we want to tie model addresses like <code class="highlighter-rouge">(:y, 4)</code> to data set
values like <code class="highlighter-rouge">ys[4]</code>:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span><span class="nf"> do_inference</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="n">amount_of_computation</span><span class="x">)</span>
    
    <span class="c"># Create a choice map that maps model addresses (:y, i)</span>
    <span class="c"># to observed values ys[i]. We leave :slope and :intercept</span>
    <span class="c"># unconstrained, because we want them to be inferred.</span>
    <span class="n">observations</span> <span class="o">=</span> <span class="n">Gen</span><span class="o">.</span><span class="n">choicemap</span><span class="x">()</span>
    <span class="k">for</span> <span class="x">(</span><span class="n">i</span><span class="x">,</span> <span class="n">y</span><span class="x">)</span> <span class="k">in</span> <span class="n">enumerate</span><span class="x">(</span><span class="n">ys</span><span class="x">)</span>
        <span class="n">observations</span><span class="x">[(:</span><span class="n">y</span><span class="x">,</span> <span class="n">i</span><span class="x">)]</span> <span class="o">=</span> <span class="n">y</span>
    <span class="k">end</span>
    
    <span class="c"># Call importance_resampling to obtain a likely trace consistent</span>
    <span class="c"># with our observations.</span>
    <span class="x">(</span><span class="n">trace</span><span class="x">,</span> <span class="n">_</span><span class="x">)</span> <span class="o">=</span> <span class="n">Gen</span><span class="o">.</span><span class="n">importance_resampling</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="x">(</span><span class="n">xs</span><span class="x">,),</span> <span class="n">observations</span><span class="x">,</span> <span class="n">amount_of_computation</span><span class="x">);</span>
    <span class="k">return</span> <span class="n">trace</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<p>We can run the inference program to obtain a trace, and then visualize the result:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trace</span> <span class="o">=</span> <span class="n">do_inference</span><span class="x">(</span><span class="n">line_model</span><span class="x">,</span> <span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="mi">100</span><span class="x">)</span>
<span class="n">render_trace</span><span class="x">(</span><span class="n">trace</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_57_0.svg" alt="svg" /></p>

<p>We see that <code class="highlighter-rouge">importance_resampling</code> found a reasonable slope and intercept to explain the data. We can also visualize many samples in a grid:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">traces</span> <span class="o">=</span> <span class="x">[</span><span class="n">do_inference</span><span class="x">(</span><span class="n">line_model</span><span class="x">,</span> <span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="mi">100</span><span class="x">)</span> <span class="k">for</span> <span class="n">_</span><span class="o">=</span><span class="mi">1</span><span class="x">:</span><span class="mi">10</span><span class="x">];</span>
<span class="n">grid</span><span class="x">(</span><span class="n">render_trace</span><span class="x">,</span> <span class="n">traces</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_59_0.svg" alt="svg" /></p>

<p>We can see here that there is some uncertainty: with our limited data, we can’t be 100% sure exactly where the line is. We can get a better sense for the variability in the posterior distribution by visualizing all the traces in one plot, rather than in a grid. Each trace is going to have the same observed data points, so we only plot those once, based on the values in the first trace:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span><span class="nf"> overlay</span><span class="x">(</span><span class="n">renderer</span><span class="x">,</span> <span class="n">traces</span><span class="x">;</span> <span class="n">same_data</span><span class="o">=</span><span class="n">true</span><span class="x">,</span> <span class="n">args</span><span class="o">...</span><span class="x">)</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">renderer</span><span class="x">(</span><span class="n">traces</span><span class="x">[</span><span class="mi">1</span><span class="x">],</span> <span class="n">show_data</span><span class="o">=</span><span class="n">true</span><span class="x">,</span> <span class="n">args</span><span class="o">...</span><span class="x">)</span>
    
    <span class="n">xs</span><span class="x">,</span> <span class="o">=</span> <span class="n">get_args</span><span class="x">(</span><span class="n">traces</span><span class="x">[</span><span class="mi">1</span><span class="x">])</span>
    <span class="n">xmin</span> <span class="o">=</span> <span class="n">minimum</span><span class="x">(</span><span class="n">xs</span><span class="x">)</span>
    <span class="n">xmax</span> <span class="o">=</span> <span class="n">maximum</span><span class="x">(</span><span class="n">xs</span><span class="x">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="o">=</span><span class="mi">2</span><span class="x">:</span><span class="n">length</span><span class="x">(</span><span class="n">traces</span><span class="x">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">get_retval</span><span class="x">(</span><span class="n">traces</span><span class="x">[</span><span class="n">i</span><span class="x">])</span>
        <span class="n">test_xs</span> <span class="o">=</span> <span class="n">collect</span><span class="x">(</span><span class="n">range</span><span class="x">(</span><span class="o">-</span><span class="mi">5</span><span class="x">,</span> <span class="n">stop</span><span class="o">=</span><span class="mi">5</span><span class="x">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">1000</span><span class="x">))</span>
        <span class="n">fig</span> <span class="o">=</span> <span class="n">plot!</span><span class="x">(</span><span class="n">test_xs</span><span class="x">,</span> <span class="n">map</span><span class="x">(</span><span class="n">y</span><span class="x">,</span> <span class="n">test_xs</span><span class="x">),</span> <span class="n">color</span><span class="o">=</span><span class="s">"black"</span><span class="x">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="x">,</span> <span class="n">label</span><span class="o">=</span><span class="n">nothing</span><span class="x">,</span>
                    <span class="n">xlim</span><span class="o">=</span><span class="x">(</span><span class="n">xmin</span><span class="x">,</span> <span class="n">xmax</span><span class="x">),</span> <span class="n">ylim</span><span class="o">=</span><span class="x">(</span><span class="n">xmin</span><span class="x">,</span> <span class="n">xmax</span><span class="x">))</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="n">fig</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">traces</span> <span class="o">=</span> <span class="x">[</span><span class="n">do_inference</span><span class="x">(</span><span class="n">line_model</span><span class="x">,</span> <span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="mi">100</span><span class="x">)</span> <span class="k">for</span> <span class="n">_</span><span class="o">=</span><span class="mi">1</span><span class="x">:</span><span class="mi">10</span><span class="x">];</span>
<span class="n">overlay</span><span class="x">(</span><span class="n">render_trace</span><span class="x">,</span> <span class="n">traces</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_62_0.svg" alt="svg" /></p>

<hr />

<h3 id="exercise-2">Exercise</h3>

<p>The results above were obtained for <code class="highlighter-rouge">amount_of_computation = 100</code>. Run the algorithm with this value set to <code class="highlighter-rouge">1</code>, <code class="highlighter-rouge">10</code>, and <code class="highlighter-rouge">1000</code>, etc.  Which value seems like a good tradeoff between accuracy and running time? Discuss.</p>

<hr />

<h3 id="exercise-3">Exercise</h3>
<p>Consider the following data set.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ys_sine</span> <span class="o">=</span> <span class="x">[</span><span class="mf">2.89</span><span class="x">,</span> <span class="mf">2.22</span><span class="x">,</span> <span class="o">-</span><span class="mf">0.612</span><span class="x">,</span> <span class="o">-</span><span class="mf">0.522</span><span class="x">,</span> <span class="o">-</span><span class="mf">2.65</span><span class="x">,</span> <span class="o">-</span><span class="mf">0.133</span><span class="x">,</span> <span class="mf">2.70</span><span class="x">,</span> <span class="mf">2.77</span><span class="x">,</span> <span class="mf">0.425</span><span class="x">,</span> <span class="o">-</span><span class="mf">2.11</span><span class="x">,</span> <span class="o">-</span><span class="mf">2.76</span><span class="x">];</span>
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">scatter</span><span class="x">(</span><span class="n">xs</span><span class="x">,</span> <span class="n">ys_sine</span><span class="x">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"black"</span><span class="x">,</span> <span class="n">label</span><span class="o">=</span><span class="n">nothing</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_66_0.svg" alt="svg" /></p>

<p>Write an inference program that generates traces of <code class="highlighter-rouge">sine_model</code> that explain this data set. Visualize the resulting distribution of traces. Temporarily change the prior distribution on the period to be <code class="highlighter-rouge">gamma(1, 1)</code>  (by changing and re-running the cell that defines <code class="highlighter-rouge">sine_model</code> from a previous exercise). Can you explain the difference in inference results when using <code class="highlighter-rouge">gamma(1, 1)</code> vs <code class="highlighter-rouge">gamma(5, 1)</code> prior on the period? How much computation did you need to get good results?</p>

<h2 id="4-predicting-new-data--">4. Predicting new data  <a name="predicting-data"></a></h2>

<p>What if we’d want to predict <code class="highlighter-rouge">ys</code> given <code class="highlighter-rouge">xs</code>?</p>

<p>Using the API method
<a href="https://www.gen.dev/dev/ref/gfi/#Gen.generate"><code class="highlighter-rouge">generate</code></a>, we
can generate a trace of a generative function in which the values of certain
random choices are constrained to given values. The constraints are a choice
map that maps the addresses of the constrained random choices to their
desired values.</p>

<p>For example:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">constraints</span> <span class="o">=</span> <span class="n">Gen</span><span class="o">.</span><span class="n">choicemap</span><span class="x">()</span>
<span class="n">constraints</span><span class="x">[:</span><span class="n">slope</span><span class="x">]</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">constraints</span><span class="x">[:</span><span class="n">intercept</span><span class="x">]</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="x">(</span><span class="n">trace</span><span class="x">,</span> <span class="n">_</span><span class="x">)</span> <span class="o">=</span> <span class="n">Gen</span><span class="o">.</span><span class="n">generate</span><span class="x">(</span><span class="n">line_model</span><span class="x">,</span> <span class="x">(</span><span class="n">xs</span><span class="x">,),</span> <span class="n">constraints</span><span class="x">)</span>
<span class="n">render_trace</span><span class="x">(</span><span class="n">trace</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_69_0.svg" alt="svg" /></p>

<p>Note that the random choices corresponding to the y-coordinates are still made randomly. Run the cell above a few times to verify this.</p>

<p>We will use the ability to run constrained executions of a generative
function to predict the value of the y-coordinates at new x-coordinates by
running new executions of the model generative function in which the random
choices corresponding to the parameters have been constrained to their
inferred values.  We have provided a function below (<code class="highlighter-rouge">predict_new_data</code>) that
takes a trace, and a vector of new x-coordinates, and returns a vector of
predicted y-coordinates corresponding to the x-coordinates in <code class="highlighter-rouge">new_xs</code>. We
have designed this function to work with multiple models, so the set of
parameter addresses is an argument (<code class="highlighter-rouge">param_addrs</code>):</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span><span class="nf"> predict_new_data</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">trace</span><span class="x">,</span> <span class="n">new_xs</span><span class="o">::</span><span class="n">Vector</span><span class="x">{</span><span class="kt">Float64</span><span class="x">},</span> <span class="n">param_addrs</span><span class="x">)</span>
    
    <span class="c"># Copy parameter values from the inferred trace (`trace`)</span>
    <span class="c"># into a fresh set of constraints.</span>
    <span class="n">constraints</span> <span class="o">=</span> <span class="n">Gen</span><span class="o">.</span><span class="n">choicemap</span><span class="x">()</span>
    <span class="k">for</span> <span class="n">addr</span> <span class="k">in</span> <span class="n">param_addrs</span>
        <span class="n">constraints</span><span class="x">[</span><span class="n">addr</span><span class="x">]</span> <span class="o">=</span> <span class="n">trace</span><span class="x">[</span><span class="n">addr</span><span class="x">]</span>
    <span class="k">end</span>
    
    <span class="c"># Run the model with new x coordinates, and with parameters </span>
    <span class="c"># fixed to be the inferred values.</span>
    <span class="x">(</span><span class="n">new_trace</span><span class="x">,</span> <span class="n">_</span><span class="x">)</span> <span class="o">=</span> <span class="n">Gen</span><span class="o">.</span><span class="n">generate</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="x">(</span><span class="n">new_xs</span><span class="x">,),</span> <span class="n">constraints</span><span class="x">)</span>
    
    <span class="c"># Pull out the y-values and return them.</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="x">[</span><span class="n">new_trace</span><span class="x">[(:</span><span class="n">y</span><span class="x">,</span> <span class="n">i</span><span class="x">)]</span> <span class="k">for</span> <span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="x">:</span><span class="n">length</span><span class="x">(</span><span class="n">new_xs</span><span class="x">)]</span>
    <span class="k">return</span> <span class="n">ys</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<p>To illustrate, we call the function above given the previous trace (which
constrained slope and intercept to be zero).</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predict_new_data</span><span class="x">(</span><span class="n">line_model</span><span class="x">,</span> <span class="n">trace</span><span class="x">,</span> <span class="x">[</span><span class="mf">1.</span><span class="x">,</span> <span class="mf">2.</span><span class="x">,</span> <span class="mf">3.</span><span class="x">],</span> <span class="x">[:</span><span class="n">slope</span><span class="x">,</span> <span class="x">:</span><span class="n">intercept</span><span class="x">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>3-element Vector{Float64}:
  0.02694273341948937
 -0.05578345643577398
  0.0031286541463625053
</code></pre></div></div>

<p>The cell below defines a function that first performs inference on an
observed data set <code class="highlighter-rouge">(xs, ys)</code>, and then runs <code class="highlighter-rouge">predict_new_data</code> to generate
predicted y-coordinates. It repeats this process <code class="highlighter-rouge">num_traces</code> times, and
returns a vector of the resulting y-coordinate vectors.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span><span class="nf"> infer_and_predict</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="n">param_addrs</span><span class="x">,</span> <span class="n">num_traces</span><span class="x">,</span> <span class="n">amount_of_computation</span><span class="x">)</span>
    <span class="n">pred_ys</span> <span class="o">=</span> <span class="x">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="x">:</span><span class="n">num_traces</span>
        <span class="n">trace</span> <span class="o">=</span> <span class="n">do_inference</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="n">amount_of_computation</span><span class="x">)</span>
        <span class="n">push!</span><span class="x">(</span><span class="n">pred_ys</span><span class="x">,</span> <span class="n">predict_new_data</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">trace</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="n">param_addrs</span><span class="x">))</span>
    <span class="k">end</span>
    <span class="n">pred_ys</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<p>To illustrate, we generate predictions at <code class="highlighter-rouge">[1., 2., 3.]</code> given one (approximate) posterior
trace.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pred_ys</span> <span class="o">=</span> <span class="n">infer_and_predict</span><span class="x">(</span><span class="n">line_model</span><span class="x">,</span> <span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="x">[</span><span class="mf">1.</span><span class="x">,</span> <span class="mf">2.</span><span class="x">,</span> <span class="mf">3.</span><span class="x">],</span> <span class="x">[:</span><span class="n">slope</span><span class="x">,</span> <span class="x">:</span><span class="n">intercept</span><span class="x">],</span> <span class="mi">1</span><span class="x">,</span> <span class="mi">1000</span><span class="x">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1-element Vector{Any}:
 [1.0032271405440183, -0.13506083352207435, -0.9583356345851033]
</code></pre></div></div>

<p>Finally, we define a cell that plots the observed data set <code class="highlighter-rouge">(xs, ys)</code> as red dots, and the predicted data as small black dots.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span><span class="nf"> plot_predictions</span><span class="x">(</span><span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="n">pred_ys</span><span class="x">;</span> <span class="n">title</span><span class="o">=</span><span class="s">"predictions"</span><span class="x">)</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">scatter</span><span class="x">(</span><span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"red"</span><span class="x">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"observed data"</span><span class="x">,</span> <span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="x">)</span>
    <span class="k">for</span> <span class="x">(</span><span class="n">i</span><span class="x">,</span> <span class="n">pred_ys_single</span><span class="x">)</span> <span class="k">in</span> <span class="n">enumerate</span><span class="x">(</span><span class="n">pred_ys</span><span class="x">)</span>
        <span class="n">scatter!</span><span class="x">(</span><span class="n">new_xs</span><span class="x">,</span> <span class="n">pred_ys_single</span><span class="x">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"black"</span><span class="x">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="x">,</span> <span class="n">label</span><span class="o">=</span><span class="n">i</span> <span class="o">==</span> <span class="mi">1</span> <span class="o">?</span> <span class="s">"predictions"</span> <span class="x">:</span> <span class="n">nothing</span><span class="x">)</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="n">fig</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<p>Recall the original dataset for the line model. The x-coordinates span the interval -5 to 5.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">scatter</span><span class="x">(</span><span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"red"</span><span class="x">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"observed data"</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_82_0.svg" alt="svg" /></p>

<p>We will use the inferred values of the parameters to predict y-coordinates for x-coordinates in the interval 5 to 10 from which data was not observed. We will also predict new data within the interval -5 to 5, and we will compare this data to the original observed data. Predicting new data from inferred parameters, and comparing this new data to the observed data is the core idea behind <em>posterior predictive checking</em>. This tutorial does not intend to give a rigorous overview behind techniques for checking the quality of a model, but intends to give high-level intuition.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_xs</span> <span class="o">=</span> <span class="n">collect</span><span class="x">(</span><span class="n">range</span><span class="x">(</span><span class="o">-</span><span class="mi">5</span><span class="x">,</span> <span class="n">stop</span><span class="o">=</span><span class="mi">10</span><span class="x">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">100</span><span class="x">));</span>
</code></pre></div></div>

<p>We generate and plot the predicted data:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pred_ys</span> <span class="o">=</span> <span class="n">infer_and_predict</span><span class="x">(</span><span class="n">line_model</span><span class="x">,</span> <span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="x">[:</span><span class="n">slope</span><span class="x">,</span> <span class="x">:</span><span class="n">intercept</span><span class="x">],</span> <span class="mi">20</span><span class="x">,</span> <span class="mi">1000</span><span class="x">)</span>
<span class="n">plot_predictions</span><span class="x">(</span><span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="n">pred_ys</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_86_0.svg" alt="svg" /></p>

<p>The results look reasonable, both within the interval of observed data and in the extrapolated predictions on the right.</p>

<p>Now consider the same experiment run with the following data set, which has significantly more noise.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ys_noisy</span> <span class="o">=</span> <span class="x">[</span><span class="mf">5.092</span><span class="x">,</span> <span class="mf">4.781</span><span class="x">,</span> <span class="mf">2.46815</span><span class="x">,</span> <span class="mf">1.23047</span><span class="x">,</span> <span class="mf">0.903318</span><span class="x">,</span> <span class="mf">1.11819</span><span class="x">,</span> <span class="mf">2.10808</span><span class="x">,</span> <span class="mf">1.09198</span><span class="x">,</span> <span class="mf">0.0203789</span><span class="x">,</span> <span class="o">-</span><span class="mf">2.05068</span><span class="x">,</span> <span class="mf">2.66031</span><span class="x">];</span>
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pred_ys</span> <span class="o">=</span> <span class="n">infer_and_predict</span><span class="x">(</span><span class="n">line_model</span><span class="x">,</span> <span class="n">xs</span><span class="x">,</span> <span class="n">ys_noisy</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="x">[:</span><span class="n">slope</span><span class="x">,</span> <span class="x">:</span><span class="n">intercept</span><span class="x">],</span> <span class="mi">20</span><span class="x">,</span> <span class="mi">1000</span><span class="x">)</span>
<span class="n">plot_predictions</span><span class="x">(</span><span class="n">xs</span><span class="x">,</span> <span class="n">ys_noisy</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="n">pred_ys</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_90_0.svg" alt="svg" /></p>

<p>It looks like the generated data is less noisy than the observed data in the regime where data was observed, and it looks like the forecasted data is too overconfident. This is a sign that our model is mis-specified. In our case, this is because we have assumed that the noise has value 0.1. However, the actual noise in the data appears to be much larger. We can correct this by making the noise a random choice as well and inferring its value along with the other parameters.</p>

<p>We first write a new version of the line model that samples a random choice for the noise from a <code class="highlighter-rouge">gamma(1, 1)</code> prior distribution.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@gen</span> <span class="k">function</span><span class="nf"> line_model_fancy</span><span class="x">(</span><span class="n">xs</span><span class="o">::</span><span class="n">Vector</span><span class="x">{</span><span class="kt">Float64</span><span class="x">})</span>
    <span class="n">slope</span> <span class="o">=</span> <span class="x">({:</span><span class="n">slope</span><span class="x">}</span> <span class="o">~</span> <span class="n">normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="mi">1</span><span class="x">))</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="x">({:</span><span class="n">intercept</span><span class="x">}</span> <span class="o">~</span> <span class="n">normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="mi">2</span><span class="x">))</span>
    
    <span class="k">function</span><span class="nf"> y</span><span class="x">(</span><span class="n">x</span><span class="x">)</span>
        <span class="k">return</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">intercept</span>
    <span class="k">end</span>
    
    <span class="n">noise</span> <span class="o">=</span> <span class="x">({:</span><span class="n">noise</span><span class="x">}</span> <span class="o">~</span> <span class="n">gamma</span><span class="x">(</span><span class="mi">1</span><span class="x">,</span> <span class="mi">1</span><span class="x">))</span>
    <span class="k">for</span> <span class="x">(</span><span class="n">i</span><span class="x">,</span> <span class="n">x</span><span class="x">)</span> <span class="k">in</span> <span class="n">enumerate</span><span class="x">(</span><span class="n">xs</span><span class="x">)</span>
        <span class="x">{(:</span><span class="n">y</span><span class="x">,</span> <span class="n">i</span><span class="x">)}</span> <span class="o">~</span> <span class="n">normal</span><span class="x">(</span><span class="n">slope</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">intercept</span><span class="x">,</span> <span class="n">noise</span><span class="x">)</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="n">y</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<p>Then, we compare the predictions using inference of the unmodified and modified models on the <code class="highlighter-rouge">ys</code> data set:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pred_ys</span> <span class="o">=</span> <span class="n">infer_and_predict</span><span class="x">(</span><span class="n">line_model</span><span class="x">,</span> <span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="x">[:</span><span class="n">slope</span><span class="x">,</span> <span class="x">:</span><span class="n">intercept</span><span class="x">],</span> <span class="mi">20</span><span class="x">,</span> <span class="mi">1000</span><span class="x">)</span>
<span class="n">fixed_noise_plot</span> <span class="o">=</span> <span class="n">plot_predictions</span><span class="x">(</span><span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="n">pred_ys</span><span class="x">;</span> <span class="n">title</span><span class="o">=</span><span class="s">"fixed noise"</span><span class="x">)</span>

<span class="n">pred_ys</span> <span class="o">=</span> <span class="n">infer_and_predict</span><span class="x">(</span><span class="n">line_model_fancy</span><span class="x">,</span> <span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="x">[:</span><span class="n">slope</span><span class="x">,</span> <span class="x">:</span><span class="n">intercept</span><span class="x">,</span> <span class="x">:</span><span class="n">noise</span><span class="x">],</span> <span class="mi">20</span><span class="x">,</span> <span class="mi">10000</span><span class="x">)</span>
<span class="n">inferred_noise_plot</span> <span class="o">=</span> <span class="n">plot_predictions</span><span class="x">(</span><span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="n">pred_ys</span><span class="x">;</span> <span class="n">title</span><span class="o">=</span><span class="s">"inferred noise"</span><span class="x">)</span>

<span class="n">plot</span><span class="x">(</span><span class="n">fixed_noise_plot</span><span class="x">,</span> <span class="n">inferred_noise_plot</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_95_0.svg" alt="svg" /></p>

<p>Notice that there is more uncertainty in the predictions made using the modified model.</p>

<p>We also compare the predictions using inference of the unmodified and modified models on the <code class="highlighter-rouge">ys_noisy</code> data set:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pred_ys</span> <span class="o">=</span> <span class="n">infer_and_predict</span><span class="x">(</span><span class="n">line_model</span><span class="x">,</span> <span class="n">xs</span><span class="x">,</span> <span class="n">ys_noisy</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="x">[:</span><span class="n">slope</span><span class="x">,</span> <span class="x">:</span><span class="n">intercept</span><span class="x">],</span> <span class="mi">20</span><span class="x">,</span> <span class="mi">1000</span><span class="x">)</span>
<span class="n">fixed_noise_plot</span> <span class="o">=</span> <span class="n">plot_predictions</span><span class="x">(</span><span class="n">xs</span><span class="x">,</span> <span class="n">ys_noisy</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="n">pred_ys</span><span class="x">;</span> <span class="n">title</span><span class="o">=</span><span class="s">"fixed noise"</span><span class="x">)</span>


<span class="n">pred_ys</span> <span class="o">=</span> <span class="n">infer_and_predict</span><span class="x">(</span><span class="n">line_model_fancy</span><span class="x">,</span> <span class="n">xs</span><span class="x">,</span> <span class="n">ys_noisy</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="x">[:</span><span class="n">slope</span><span class="x">,</span> <span class="x">:</span><span class="n">intercept</span><span class="x">,</span> <span class="x">:</span><span class="n">noise</span><span class="x">],</span> <span class="mi">20</span><span class="x">,</span> <span class="mi">10000</span><span class="x">)</span>
<span class="n">inferred_noise_plot</span> <span class="o">=</span> <span class="n">plot_predictions</span><span class="x">(</span><span class="n">xs</span><span class="x">,</span> <span class="n">ys_noisy</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="n">pred_ys</span><span class="x">;</span> <span class="n">title</span><span class="o">=</span><span class="s">"inferred noise"</span><span class="x">)</span>

<span class="n">plot</span><span class="x">(</span><span class="n">fixed_noise_plot</span><span class="x">,</span> <span class="n">inferred_noise_plot</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_97_0.svg" alt="svg" /></p>

<p>Notice that while the unmodified model was very overconfident, the modified model has an appropriate level of uncertainty, while still capturing the general negative trend.</p>

<hr />
<h3 id="exercise-4">Exercise</h3>

<p>Write a modified version of the sine model that makes noise into a random choice. Compare the predicted data with the observed data using <code class="highlighter-rouge">infer_and_predict</code> and <code class="highlighter-rouge">plot_predictions</code> for the unmodified and modified models, and for the <code class="highlighter-rouge">ys_sine</code> and <code class="highlighter-rouge">ys_noisy</code> data sets. Discuss the results. Experiment with the amount of inference computation used. The amount of inference computation will need to be higher for the model with the noise as a random choice.</p>

<p>We have provided you with starter code:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@gen</span> <span class="k">function</span><span class="nf"> sine_model_fancy</span><span class="x">(</span><span class="n">xs</span><span class="o">::</span><span class="n">Vector</span><span class="x">{</span><span class="kt">Float64</span><span class="x">})</span>

    <span class="c"># &lt; your code here &gt;</span>

    <span class="k">for</span> <span class="x">(</span><span class="n">i</span><span class="x">,</span> <span class="n">x</span><span class="x">)</span> <span class="k">in</span> <span class="n">enumerate</span><span class="x">(</span><span class="n">xs</span><span class="x">)</span>
        <span class="x">{(:</span><span class="n">y</span><span class="x">,</span> <span class="n">i</span><span class="x">)}</span> <span class="o">~</span> <span class="n">normal</span><span class="x">(</span><span class="mf">0.</span><span class="x">,</span> <span class="mf">0.1</span><span class="x">)</span> <span class="c"># &lt; edit this line &gt;</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="n">nothing</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Modify the line below to experiment with the amount_of_computation parameter</span>
<span class="n">pred_ys</span> <span class="o">=</span> <span class="n">infer_and_predict</span><span class="x">(</span><span class="n">sine_model</span><span class="x">,</span> <span class="n">xs</span><span class="x">,</span> <span class="n">ys_sine</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="x">[],</span> <span class="mi">20</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>
<span class="n">fixed_noise_plot</span> <span class="o">=</span> <span class="n">plot_predictions</span><span class="x">(</span><span class="n">xs</span><span class="x">,</span> <span class="n">ys_sine</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="n">pred_ys</span><span class="x">;</span> <span class="n">title</span><span class="o">=</span><span class="s">"Fixed noise level"</span><span class="x">)</span>

<span class="c"># Modify the line below to experiment with the amount_of_computation parameter</span>
<span class="n">pred_ys</span> <span class="o">=</span> <span class="n">infer_and_predict</span><span class="x">(</span><span class="n">sine_model_fancy</span><span class="x">,</span> <span class="n">xs</span><span class="x">,</span> <span class="n">ys_sine</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="x">[],</span> <span class="mi">20</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>
<span class="n">inferred_noise_plot</span> <span class="o">=</span> <span class="n">plot_predictions</span><span class="x">(</span><span class="n">xs</span><span class="x">,</span> <span class="n">ys_sine</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="n">pred_ys</span><span class="x">;</span> <span class="n">title</span><span class="o">=</span><span class="s">"Inferred noise level"</span><span class="x">)</span>

<span class="n">Plots</span><span class="o">.</span><span class="n">plot</span><span class="x">(</span><span class="n">fixed_noise_plot</span><span class="x">,</span> <span class="n">inferred_noise_plot</span><span class="x">)</span>
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Modify the line below to experiment with the amount_of_computation parameter</span>
<span class="n">pred_ys</span> <span class="o">=</span> <span class="n">infer_and_predict</span><span class="x">(</span><span class="n">sine_model</span><span class="x">,</span> <span class="n">xs</span><span class="x">,</span> <span class="n">ys_noisy</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="x">[],</span> <span class="mi">20</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>
<span class="n">fixed_noise_plot</span> <span class="o">=</span> <span class="n">plot_predictions</span><span class="x">(</span><span class="n">xs</span><span class="x">,</span> <span class="n">ys_noisy</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="n">pred_ys</span><span class="x">;</span> <span class="n">title</span><span class="o">=</span><span class="s">"Fixed noise level"</span><span class="x">)</span>

<span class="c"># Modify the line below to experiment with the amount_of_computation parameter</span>
<span class="n">pred_ys</span> <span class="o">=</span> <span class="n">infer_and_predict</span><span class="x">(</span><span class="n">sine_model_fancy</span><span class="x">,</span> <span class="n">xs</span><span class="x">,</span> <span class="n">ys_noisy</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="x">[],</span> <span class="mi">20</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>
<span class="n">inferred_noise_plot</span> <span class="o">=</span> <span class="n">plot_predictions</span><span class="x">(</span><span class="n">xs</span><span class="x">,</span> <span class="n">ys_noisy</span><span class="x">,</span> <span class="n">new_xs</span><span class="x">,</span> <span class="n">pred_ys</span><span class="x">;</span> <span class="n">title</span><span class="o">=</span><span class="s">"Inferred noise level"</span><span class="x">)</span>

<span class="n">Plots</span><span class="o">.</span><span class="n">plot</span><span class="x">(</span><span class="n">fixed_noise_plot</span><span class="x">,</span> <span class="n">inferred_noise_plot</span><span class="x">)</span>
</code></pre></div></div>

<hr />

<!-- # Solution
@gen function sine_model_fancy(xs::Vector{Float64})
    period = ({:period} ~ gamma(5, 1))
    amplitude = ({:amplitude} ~ gamma(1, 1))
    phase = ({:phase} ~ uniform(0, 2*pi))
    noise = ({:noise} ~ gamma(1, 1))

    function y(x)
        return amplitude * sin(x * (2 * pi / period) + phase)
    end

    for (i, x) in enumerate(xs)
        {(:y, i)} ~ normal(y(x), noise)
    end

    return y
end; -->

<h2 id="5-calling-other-generative-functions--">5. Calling other generative functions  <a name="calling-functions"></a></h2>

<p>In addition to making random choices, generative functions can invoke other
generative functions. To illustrate this, we will write a probabilistic model
that combines the line model and the sine model. This model is able to
explain data using either model, and which model is chosen will depend on the
data. This is called <em>model selection</em>.</p>

<p>A generative function can invoke another generative function in three ways:</p>

<ul>
  <li>
    <p><strong>(NOT RECOMMENDED)</strong> using regular Julia function call syntax: <code class="highlighter-rouge">f(x)</code></p>
  </li>
  <li>
    <p>using the <code class="highlighter-rouge">~</code> snytax with an address for the call: <code class="highlighter-rouge">{addr} ~ f(x)</code></p>
  </li>
  <li>
    <p>using the <code class="highlighter-rouge">~</code> syntax with a wildcard address: <code class="highlighter-rouge">{*} ~ f(x)</code></p>
  </li>
</ul>

<p>When invoking using regular function call syntax, the random choices made by
the callee function are not traced at all, and Gen cannot reason about them during inference. 
When invoking using <code class="highlighter-rouge">~</code> with a <em>wildcard</em> address (<code class="highlighter-rouge">{*} ~ f(x)</code>), the random choices of the 
callee function are imported directly into the caller’s trace. So, for example, if <code class="highlighter-rouge">f</code> makes a choice
called <code class="highlighter-rouge">:f_choice</code>, then the caller’s trace will have a choice called <code class="highlighter-rouge">:f_choice</code> too. 
Note that a downside of this is that if <code class="highlighter-rouge">f</code> is called <em>twice</em> by the same caller, then the two 
choices called <code class="highlighter-rouge">:f_choice</code> will clash, leading to an error.
In this case, it is best to provide an address (<code class="highlighter-rouge">{addr} ~ f(x)</code>): <code class="highlighter-rouge">f</code>’s random choices will
be placed under the <em>namespace</em> <code class="highlighter-rouge">addr</code>.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@gen</span> <span class="k">function</span><span class="nf"> foo</span><span class="x">()</span>
    <span class="x">{:</span><span class="n">y</span><span class="x">}</span> <span class="o">~</span> <span class="n">normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>
<span class="k">end</span>

<span class="nd">@gen</span> <span class="k">function</span><span class="nf"> bar</span><span class="x">()</span>
    <span class="x">{:</span><span class="n">x</span><span class="x">}</span> <span class="o">~</span> <span class="n">bernoulli</span><span class="x">(</span><span class="mf">0.5</span><span class="x">)</span>
    <span class="c"># Call `foo` with a wildcard address.</span>
    <span class="c"># Its choices (:y) will appear directly</span>
    <span class="c"># within the trace of `bar`.</span>
    <span class="x">{</span><span class="o">*</span><span class="x">}</span> <span class="o">~</span> <span class="n">foo</span><span class="x">()</span>
<span class="k">end</span>

<span class="nd">@gen</span> <span class="k">function</span><span class="nf"> bar_using_namespace</span><span class="x">()</span>
    <span class="x">{:</span><span class="n">x</span><span class="x">}</span> <span class="o">~</span> <span class="n">bernoulli</span><span class="x">(</span><span class="mf">0.5</span><span class="x">)</span>
    <span class="c"># Call `foo` with the address `:z`.</span>
    <span class="c"># The internal choice `:y` of `foo`</span>
    <span class="c"># will appear in our trace at the</span>
    <span class="c"># hierarchical address `:z =&gt; :y`.</span>
    <span class="x">{:</span><span class="n">z</span><span class="x">}</span> <span class="o">~</span> <span class="n">foo</span><span class="x">()</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<p>We first show the addresses sampled by <code class="highlighter-rouge">bar</code>:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trace</span> <span class="o">=</span> <span class="n">Gen</span><span class="o">.</span><span class="n">simulate</span><span class="x">(</span><span class="n">bar</span><span class="x">,</span> <span class="x">())</span>
<span class="n">Gen</span><span class="o">.</span><span class="n">get_choices</span><span class="x">(</span><span class="n">trace</span><span class="x">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>│
├── :y : 0.6475752978557953
│
└── :x : true
</code></pre></div></div>

<p>And the addresses sampled by <code class="highlighter-rouge">bar_using_namespace</code>:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trace</span> <span class="o">=</span> <span class="n">Gen</span><span class="o">.</span><span class="n">simulate</span><span class="x">(</span><span class="n">bar_using_namespace</span><span class="x">,</span> <span class="x">())</span>
<span class="n">Gen</span><span class="o">.</span><span class="n">get_choices</span><span class="x">(</span><span class="n">trace</span><span class="x">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>│
├── :x : true
│
└── :z
    │
    └── :y : 0.3801328264226968
</code></pre></div></div>

<p>Using <code class="highlighter-rouge">{addr} ~ f()</code>, with a namespace, can help avoid address collisions for complex models.</p>

<p>A hierarchical address is represented as a Julia <code class="highlighter-rouge">Pair</code>, where the first element of the pair is the first element of the address and the second element of the pair is the rest of the address:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trace</span><span class="x">[</span><span class="n">Pair</span><span class="x">(:</span><span class="n">z</span><span class="x">,</span> <span class="x">:</span><span class="n">y</span><span class="x">)]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.3801328264226968
</code></pre></div></div>

<p>Julia uses the <code class="highlighter-rouge">=&gt;</code> operator as a shorthand for the <code class="highlighter-rouge">Pair</code> constructor, so we can access choices at hierarchical addresses like:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trace</span><span class="x">[:</span><span class="n">z</span> <span class="o">=&gt;</span> <span class="x">:</span><span class="n">y</span><span class="x">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.3801328264226968
</code></pre></div></div>

<p>If we have a hierarchical address with more than two elements, we can construct the address by chaining the <code class="highlighter-rouge">=&gt;</code> operator:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@gen</span> <span class="k">function</span><span class="nf"> baz</span><span class="x">()</span>
    <span class="x">{:</span><span class="n">a</span><span class="x">}</span> <span class="o">~</span> <span class="n">bar_using_namespace</span><span class="x">()</span>
<span class="k">end</span>

<span class="n">trace</span> <span class="o">=</span> <span class="n">simulate</span><span class="x">(</span><span class="n">baz</span><span class="x">,</span> <span class="x">())</span>

<span class="n">trace</span><span class="x">[:</span><span class="n">a</span> <span class="o">=&gt;</span> <span class="x">:</span><span class="n">z</span> <span class="o">=&gt;</span> <span class="x">:</span><span class="n">y</span><span class="x">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.5622991768999647
</code></pre></div></div>

<p>Note that the <code class="highlighter-rouge">=&gt;</code> operator associated right, so this is equivalent to:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trace</span><span class="x">[</span><span class="n">Pair</span><span class="x">(:</span><span class="n">a</span><span class="x">,</span> <span class="n">Pair</span><span class="x">(:</span><span class="n">z</span><span class="x">,</span> <span class="x">:</span><span class="n">y</span><span class="x">))]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.5622991768999647
</code></pre></div></div>

<p>Now, we write a generative function that combines the line and sine models. It makes a Bernoulli random choice (e.g. a coin flip that returns true or false) that determines which of the two models will generate the data.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@gen</span> <span class="k">function</span><span class="nf"> combined_model</span><span class="x">(</span><span class="n">xs</span><span class="o">::</span><span class="n">Vector</span><span class="x">{</span><span class="kt">Float64</span><span class="x">})</span>
    <span class="k">if</span> <span class="x">({:</span><span class="n">is_line</span><span class="x">}</span> <span class="o">~</span> <span class="n">bernoulli</span><span class="x">(</span><span class="mf">0.5</span><span class="x">))</span>
        <span class="c"># Call line_model_fancy on xs, and import</span>
        <span class="c"># its random choices directly into our trace.</span>
        <span class="k">return</span> <span class="x">({</span><span class="o">*</span><span class="x">}</span> <span class="o">~</span> <span class="n">line_model_fancy</span><span class="x">(</span><span class="n">xs</span><span class="x">))</span>
    <span class="k">else</span>
        <span class="c"># Call sine_model_fancy on xs, and import</span>
        <span class="c"># its random choices directly into our trace</span>
        <span class="k">return</span> <span class="x">({</span><span class="o">*</span><span class="x">}</span> <span class="o">~</span> <span class="n">sine_model_fancy</span><span class="x">(</span><span class="n">xs</span><span class="x">))</span>
    <span class="k">end</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<p>We visualize some traces, and see that sometimes it samples linear data and other times sinusoidal data.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">traces</span> <span class="o">=</span> <span class="x">[</span><span class="n">Gen</span><span class="o">.</span><span class="n">simulate</span><span class="x">(</span><span class="n">combined_model</span><span class="x">,</span> <span class="x">(</span><span class="n">xs</span><span class="x">,))</span> <span class="k">for</span> <span class="n">_</span><span class="o">=</span><span class="mi">1</span><span class="x">:</span><span class="mi">12</span><span class="x">];</span>
<span class="n">grid</span><span class="x">(</span><span class="n">render_trace</span><span class="x">,</span> <span class="n">traces</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_123_0.svg" alt="svg" /></p>

<p>We run inference using this combined model on the <code class="highlighter-rouge">ys</code> data set and the <code class="highlighter-rouge">ys_sine</code> data set.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">traces</span> <span class="o">=</span> <span class="x">[</span><span class="n">do_inference</span><span class="x">(</span><span class="n">combined_model</span><span class="x">,</span> <span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="mi">10000</span><span class="x">)</span> <span class="k">for</span> <span class="n">_</span><span class="o">=</span><span class="mi">1</span><span class="x">:</span><span class="mi">10</span><span class="x">];</span>
<span class="n">linear_dataset_plot</span> <span class="o">=</span> <span class="n">overlay</span><span class="x">(</span><span class="n">render_trace</span><span class="x">,</span> <span class="n">traces</span><span class="x">)</span>
<span class="n">traces</span> <span class="o">=</span> <span class="x">[</span><span class="n">do_inference</span><span class="x">(</span><span class="n">combined_model</span><span class="x">,</span> <span class="n">xs</span><span class="x">,</span> <span class="n">ys_sine</span><span class="x">,</span> <span class="mi">10000</span><span class="x">)</span> <span class="k">for</span> <span class="n">_</span><span class="o">=</span><span class="mi">1</span><span class="x">:</span><span class="mi">10</span><span class="x">];</span>
<span class="n">sine_dataset_plot</span> <span class="o">=</span> <span class="n">overlay</span><span class="x">(</span><span class="n">render_trace</span><span class="x">,</span> <span class="n">traces</span><span class="x">)</span>
<span class="n">Plots</span><span class="o">.</span><span class="n">plot</span><span class="x">(</span><span class="n">linear_dataset_plot</span><span class="x">,</span> <span class="n">sine_dataset_plot</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_125_0.svg" alt="svg" /></p>

<p>The results should show that the line model was inferred for the <code class="highlighter-rouge">ys</code> data set, and the sine wave model was inferred for the <code class="highlighter-rouge">ys_sine</code> data set.</p>

<hr />

<h3 id="exercise-5">Exercise</h3>

<p>Construct a data set for which it is ambiguous whether the line or sine wave model is best. Visualize the inferred traces using <code class="highlighter-rouge">render_combined</code> to illustrate the ambiguity. Write a program that takes the data set and returns an estimate of the posterior probability that the data was generated by the sine wave model, and run it on your data set.</p>

<p>Hint: To estimate the posterior probability that the data was generated by the sine wave model, run the inference program many times to compute a large number of traces, and then compute the fraction of those traces in which <code class="highlighter-rouge">:is_line</code> is false.</p>

<hr />

<h2 id="6-modeling-with-an-unbounded-number-of-parameters--">6. Modeling with an unbounded number of parameters  <a name="infinite-space"></a></h2>

<p>Gen’s built-in modeling language can be used to express models that use an
unbounded number of parameters. This section walks you through development of
a model of data that does not a-priori specify an upper bound on the
complexity of the model, but instead infers the complexity of the model as
well as the parameters. This is a simple example of a <em>Bayesian
nonparametric</em> model.</p>

<p>We will consider two data sets:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xs_dense</span> <span class="o">=</span> <span class="n">collect</span><span class="x">(</span><span class="n">range</span><span class="x">(</span><span class="o">-</span><span class="mi">5</span><span class="x">,</span> <span class="n">stop</span><span class="o">=</span><span class="mi">5</span><span class="x">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">50</span><span class="x">))</span>
<span class="n">ys_simple</span> <span class="o">=</span> <span class="n">fill</span><span class="x">(</span><span class="mf">1.</span><span class="x">,</span> <span class="n">length</span><span class="x">(</span><span class="n">xs_dense</span><span class="x">))</span> <span class="o">.+</span> <span class="n">randn</span><span class="x">(</span><span class="n">length</span><span class="x">(</span><span class="n">xs_dense</span><span class="x">))</span> <span class="o">*</span> <span class="mf">0.1</span>
<span class="n">ys_complex</span> <span class="o">=</span> <span class="x">[</span><span class="kt">Int</span><span class="x">(</span><span class="n">floor</span><span class="x">(</span><span class="n">abs</span><span class="x">(</span><span class="n">x</span><span class="o">/</span><span class="mi">3</span><span class="x">)))</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">?</span> <span class="mi">2</span> <span class="x">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="k">in</span> <span class="n">xs_dense</span><span class="x">]</span> <span class="o">.+</span> <span class="n">randn</span><span class="x">(</span><span class="n">length</span><span class="x">(</span><span class="n">xs_dense</span><span class="x">))</span> <span class="o">*</span> <span class="mf">0.1</span><span class="x">;</span>
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">simple_plot</span> <span class="o">=</span> <span class="n">scatter</span><span class="x">(</span><span class="n">xs_dense</span><span class="x">,</span> <span class="n">ys_simple</span><span class="x">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"black"</span><span class="x">,</span> <span class="n">label</span><span class="o">=</span><span class="n">nothing</span><span class="x">,</span> <span class="n">title</span><span class="o">=</span><span class="s">"ys-simple"</span><span class="x">,</span> <span class="n">ylim</span><span class="o">=</span><span class="x">(</span><span class="o">-</span><span class="mi">1</span><span class="x">,</span> <span class="mi">3</span><span class="x">))</span>
<span class="n">complex_plot</span> <span class="o">=</span> <span class="n">scatter</span><span class="x">(</span><span class="n">xs_dense</span><span class="x">,</span> <span class="n">ys_complex</span><span class="x">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"black"</span><span class="x">,</span> <span class="n">label</span><span class="o">=</span><span class="n">nothing</span><span class="x">,</span> <span class="n">title</span><span class="o">=</span><span class="s">"ys-complex"</span><span class="x">,</span> <span class="n">ylim</span><span class="o">=</span><span class="x">(</span><span class="o">-</span><span class="mi">1</span><span class="x">,</span> <span class="mi">3</span><span class="x">))</span>
<span class="n">Plots</span><span class="o">.</span><span class="n">plot</span><span class="x">(</span><span class="n">simple_plot</span><span class="x">,</span> <span class="n">complex_plot</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_132_0.svg" alt="svg" /></p>

<p>The data set on the left appears to be best explained as a contant function
with some noise. The data set on the right appears to include two
changepoints, with a constant function in between the changepoints. We want a
model that does not a-priori choose the number of changepoints in the data.
To do this, we will recursively partition the interval into regions. We
define a Julia data structure that represents a binary tree of intervals;
each leaf node represents a region in which the function is constant.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">struct</span> <span class="n">Interval</span>
    <span class="n">l</span><span class="o">::</span><span class="kt">Float64</span>
    <span class="n">u</span><span class="o">::</span><span class="kt">Float64</span>
<span class="k">end</span>
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">abstract</span><span class="nc"> type</span> <span class="n">Node</span> <span class="k">end</span>
    
<span class="n">struct</span> <span class="n">InternalNode</span> <span class="o">&lt;:</span> <span class="n">Node</span>
    <span class="n">left</span><span class="o">::</span><span class="n">Node</span>
    <span class="n">right</span><span class="o">::</span><span class="n">Node</span>
    <span class="n">interval</span><span class="o">::</span><span class="n">Interval</span>
<span class="k">end</span>

<span class="n">struct</span> <span class="n">LeafNode</span> <span class="o">&lt;:</span> <span class="n">Node</span>
    <span class="n">value</span><span class="o">::</span><span class="kt">Float64</span>
    <span class="n">interval</span><span class="o">::</span><span class="n">Interval</span>
<span class="k">end</span>
</code></pre></div></div>

<p>We now write a generative function that randomly creates such a tree. Note the use of recursion in this function to create arbitrarily large trees representing arbitrarily many changepoints. Also note that we assign the address namespaces <code class="highlighter-rouge">:left</code> and <code class="highlighter-rouge">:right</code> to the calls made for the two recursive calls to <code class="highlighter-rouge">generate_segments</code>.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@gen</span> <span class="k">function</span><span class="nf"> generate_segments</span><span class="x">(</span><span class="n">l</span><span class="o">::</span><span class="kt">Float64</span><span class="x">,</span> <span class="n">u</span><span class="o">::</span><span class="kt">Float64</span><span class="x">)</span>
    <span class="n">interval</span> <span class="o">=</span> <span class="n">Interval</span><span class="x">(</span><span class="n">l</span><span class="x">,</span> <span class="n">u</span><span class="x">)</span>
    <span class="k">if</span> <span class="x">({:</span><span class="n">isleaf</span><span class="x">}</span> <span class="o">~</span> <span class="n">bernoulli</span><span class="x">(</span><span class="mf">0.7</span><span class="x">))</span>
        <span class="n">value</span> <span class="o">=</span> <span class="x">({:</span><span class="n">value</span><span class="x">}</span> <span class="o">~</span> <span class="n">normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="mi">1</span><span class="x">))</span>
        <span class="k">return</span> <span class="n">LeafNode</span><span class="x">(</span><span class="n">value</span><span class="x">,</span> <span class="n">interval</span><span class="x">)</span>
    <span class="k">else</span>
        <span class="n">frac</span> <span class="o">=</span> <span class="x">({:</span><span class="n">frac</span><span class="x">}</span> <span class="o">~</span> <span class="n">beta</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span> <span class="mi">2</span><span class="x">))</span>
        <span class="n">mid</span>  <span class="o">=</span> <span class="n">l</span> <span class="o">+</span> <span class="x">(</span><span class="n">u</span> <span class="o">-</span> <span class="n">l</span><span class="x">)</span> <span class="o">*</span> <span class="n">frac</span>
        <span class="c"># Call generate_segments recursively!</span>
        <span class="c"># Because we will call it twice -- one for the left </span>
        <span class="c"># child and one for the right child -- we use</span>
        <span class="c"># addresses to distinguish the calls.</span>
        <span class="n">left</span> <span class="o">=</span> <span class="x">({:</span><span class="n">left</span><span class="x">}</span> <span class="o">~</span> <span class="n">generate_segments</span><span class="x">(</span><span class="n">l</span><span class="x">,</span> <span class="n">mid</span><span class="x">))</span>
        <span class="n">right</span> <span class="o">=</span> <span class="x">({:</span><span class="n">right</span><span class="x">}</span> <span class="o">~</span> <span class="n">generate_segments</span><span class="x">(</span><span class="n">mid</span><span class="x">,</span> <span class="n">u</span><span class="x">))</span>
        <span class="k">return</span> <span class="n">InternalNode</span><span class="x">(</span><span class="n">left</span><span class="x">,</span> <span class="n">right</span><span class="x">,</span> <span class="n">interval</span><span class="x">)</span>
    <span class="k">end</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<p>We also define some helper functions to visualize traces of the <code class="highlighter-rouge">generate_segments</code> function.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span><span class="nf"> render_node</span><span class="o">!</span><span class="x">(</span><span class="n">node</span><span class="o">::</span><span class="n">LeafNode</span><span class="x">)</span>
    <span class="n">plot!</span><span class="x">([</span><span class="n">node</span><span class="o">.</span><span class="n">interval</span><span class="o">.</span><span class="n">l</span><span class="x">,</span> <span class="n">node</span><span class="o">.</span><span class="n">interval</span><span class="o">.</span><span class="n">u</span><span class="x">],</span> <span class="x">[</span><span class="n">node</span><span class="o">.</span><span class="n">value</span><span class="x">,</span> <span class="n">node</span><span class="o">.</span><span class="n">value</span><span class="x">],</span> <span class="n">label</span><span class="o">=</span><span class="n">nothing</span><span class="x">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="x">)</span>
<span class="k">end</span>

<span class="k">function</span><span class="nf"> render_node</span><span class="o">!</span><span class="x">(</span><span class="n">node</span><span class="o">::</span><span class="n">InternalNode</span><span class="x">)</span>
    <span class="n">render_node!</span><span class="x">(</span><span class="n">node</span><span class="o">.</span><span class="n">left</span><span class="x">)</span>
    <span class="n">render_node!</span><span class="x">(</span><span class="n">node</span><span class="o">.</span><span class="n">right</span><span class="x">)</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span><span class="nf"> render_segments_trace</span><span class="x">(</span><span class="n">trace</span><span class="x">;</span> <span class="n">xlim</span><span class="o">=</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span><span class="mi">1</span><span class="x">))</span>
    <span class="n">node</span> <span class="o">=</span> <span class="n">get_retval</span><span class="x">(</span><span class="n">trace</span><span class="x">)</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plot</span><span class="x">(</span><span class="n">xlim</span><span class="o">=</span><span class="n">xlim</span><span class="x">,</span> <span class="n">ylim</span><span class="o">=</span><span class="x">(</span><span class="o">-</span><span class="mi">3</span><span class="x">,</span> <span class="mi">3</span><span class="x">))</span>
    <span class="n">render_node!</span><span class="x">(</span><span class="n">node</span><span class="x">)</span>
    <span class="k">return</span> <span class="n">fig</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<p>We generate 12 traces from this function and visualize them below. We plot the piecewise constant function that was sampled by each run of the generative function. Different constant segments are shown in different colors. Run the cell a few times to get a better sense of the distribution on functions that is represented by the generative function.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">traces</span> <span class="o">=</span> <span class="x">[</span><span class="n">Gen</span><span class="o">.</span><span class="n">simulate</span><span class="x">(</span><span class="n">generate_segments</span><span class="x">,</span> <span class="x">(</span><span class="mf">0.</span><span class="x">,</span> <span class="mf">1.</span><span class="x">))</span> <span class="k">for</span> <span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="x">:</span><span class="mi">12</span><span class="x">]</span>
<span class="n">grid</span><span class="x">(</span><span class="n">render_segments_trace</span><span class="x">,</span> <span class="n">traces</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_142_0.svg" alt="svg" /></p>

<p>Because we only sub-divide an interval with 30% probability, most of these sampled traces have only one segment.</p>

<p>Now that we have a generative function that generates a random piecewise-constant function, we write a model that adds noise to the resulting constant functions to generate a data set of y-coordinates. The noise level will be a random choice.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># get_value_at searches a binary tree for</span>
<span class="c"># the leaf node containing some value.</span>
<span class="k">function</span><span class="nf"> get_value_at</span><span class="x">(</span><span class="n">x</span><span class="o">::</span><span class="kt">Float64</span><span class="x">,</span> <span class="n">node</span><span class="o">::</span><span class="n">LeafNode</span><span class="x">)</span>
    <span class="nd">@assert</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="n">node</span><span class="o">.</span><span class="n">interval</span><span class="o">.</span><span class="n">l</span> <span class="o">&amp;&amp;</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="n">node</span><span class="o">.</span><span class="n">interval</span><span class="o">.</span><span class="n">u</span>
    <span class="k">return</span> <span class="n">node</span><span class="o">.</span><span class="n">value</span>
<span class="k">end</span>

<span class="k">function</span><span class="nf"> get_value_at</span><span class="x">(</span><span class="n">x</span><span class="o">::</span><span class="kt">Float64</span><span class="x">,</span> <span class="n">node</span><span class="o">::</span><span class="n">InternalNode</span><span class="x">)</span>
    <span class="nd">@assert</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="n">node</span><span class="o">.</span><span class="n">interval</span><span class="o">.</span><span class="n">l</span> <span class="o">&amp;&amp;</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="n">node</span><span class="o">.</span><span class="n">interval</span><span class="o">.</span><span class="n">u</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="n">node</span><span class="o">.</span><span class="n">left</span><span class="o">.</span><span class="n">interval</span><span class="o">.</span><span class="n">u</span>
        <span class="n">get_value_at</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="n">node</span><span class="o">.</span><span class="n">left</span><span class="x">)</span>
    <span class="k">else</span>
        <span class="n">get_value_at</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="n">node</span><span class="o">.</span><span class="n">right</span><span class="x">)</span>
    <span class="k">end</span>
<span class="k">end</span>

<span class="c"># Our full model</span>
<span class="nd">@gen</span> <span class="k">function</span><span class="nf"> changepoint_model</span><span class="x">(</span><span class="n">xs</span><span class="o">::</span><span class="n">Vector</span><span class="x">{</span><span class="kt">Float64</span><span class="x">})</span>
    <span class="n">node</span> <span class="o">=</span> <span class="x">({:</span><span class="n">tree</span><span class="x">}</span> <span class="o">~</span> <span class="n">generate_segments</span><span class="x">(</span><span class="n">minimum</span><span class="x">(</span><span class="n">xs</span><span class="x">),</span> <span class="n">maximum</span><span class="x">(</span><span class="n">xs</span><span class="x">)))</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="x">({:</span><span class="n">noise</span><span class="x">}</span> <span class="o">~</span> <span class="n">gamma</span><span class="x">(</span><span class="mf">0.5</span><span class="x">,</span> <span class="mf">0.5</span><span class="x">))</span>
    <span class="k">for</span> <span class="x">(</span><span class="n">i</span><span class="x">,</span> <span class="n">x</span><span class="x">)</span> <span class="k">in</span> <span class="n">enumerate</span><span class="x">(</span><span class="n">xs</span><span class="x">)</span>
        <span class="x">{(:</span><span class="n">y</span><span class="x">,</span> <span class="n">i</span><span class="x">)}</span> <span class="o">~</span> <span class="n">normal</span><span class="x">(</span><span class="n">get_value_at</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="n">node</span><span class="x">),</span> <span class="n">noise</span><span class="x">)</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="n">node</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<p>We write a visualization for <code class="highlighter-rouge">changepoint_model</code> below:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span><span class="nf"> render_changepoint_model_trace</span><span class="x">(</span><span class="n">trace</span><span class="x">;</span> <span class="n">show_data</span><span class="o">=</span><span class="n">true</span><span class="x">)</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="n">Gen</span><span class="o">.</span><span class="n">get_args</span><span class="x">(</span><span class="n">trace</span><span class="x">)[</span><span class="mi">1</span><span class="x">]</span>
    <span class="n">node</span> <span class="o">=</span> <span class="n">Gen</span><span class="o">.</span><span class="n">get_retval</span><span class="x">(</span><span class="n">trace</span><span class="x">)</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">render_segments_trace</span><span class="x">(</span><span class="n">trace</span><span class="x">;</span> <span class="n">xlim</span><span class="o">=</span><span class="x">(</span><span class="n">minimum</span><span class="x">(</span><span class="n">xs</span><span class="x">),</span> <span class="n">maximum</span><span class="x">(</span><span class="n">xs</span><span class="x">)))</span>
    <span class="n">render_node!</span><span class="x">(</span><span class="n">node</span><span class="x">)</span>
    <span class="k">if</span> <span class="n">show_data</span>
        <span class="n">ys</span> <span class="o">=</span> <span class="x">[</span><span class="n">trace</span><span class="x">[(:</span><span class="n">y</span><span class="x">,</span> <span class="n">i</span><span class="x">)]</span> <span class="k">for</span> <span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="x">:</span><span class="n">length</span><span class="x">(</span><span class="n">xs</span><span class="x">)]</span>
        <span class="n">scatter!</span><span class="x">(</span><span class="n">xs</span><span class="x">,</span> <span class="n">ys</span><span class="x">,</span> <span class="n">c</span><span class="o">=</span><span class="s">"gray"</span><span class="x">,</span> <span class="n">label</span><span class="o">=</span><span class="n">nothing</span><span class="x">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="x">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">3</span><span class="x">)</span>
    <span class="k">end</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<p>Finally, we generate some simulated data sets and visualize them on top of the underlying piecewise constant function from which they were generated:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">traces</span> <span class="o">=</span> <span class="x">[</span><span class="n">Gen</span><span class="o">.</span><span class="n">simulate</span><span class="x">(</span><span class="n">changepoint_model</span><span class="x">,</span> <span class="x">(</span><span class="n">xs_dense</span><span class="x">,))</span> <span class="k">for</span> <span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="x">:</span><span class="mi">12</span><span class="x">]</span>
<span class="n">grid</span><span class="x">(</span><span class="n">render_changepoint_model_trace</span><span class="x">,</span> <span class="n">traces</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_149_0.svg" alt="svg" /></p>

<p>Notice that the amount of variability around the piecewise constant mean function differs from trace to trace.</p>

<p>Now we perform inference for the simple data set:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">traces</span> <span class="o">=</span> <span class="x">[</span><span class="n">do_inference</span><span class="x">(</span><span class="n">changepoint_model</span><span class="x">,</span> <span class="n">xs_dense</span><span class="x">,</span> <span class="n">ys_simple</span><span class="x">,</span> <span class="mi">10000</span><span class="x">)</span> <span class="k">for</span> <span class="n">_</span><span class="o">=</span><span class="mi">1</span><span class="x">:</span><span class="mi">12</span><span class="x">];</span>
<span class="n">grid</span><span class="x">(</span><span class="n">render_changepoint_model_trace</span><span class="x">,</span> <span class="n">traces</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_152_0.svg" alt="svg" /></p>

<p>We see that we inferred that the mean function that explains the data is a constant with very high probability.</p>

<p>For inference about the complex data set, we use more computation. You can experiment with different amounts of computation to see how the quality of the inferences degrade with less computation. Note that we are using a very simple generic inference algorithm in this tutorial, which really isn’t suited for this more complex task. In later tutorials, we will learn how to write more efficient algorithms, so that accurate results can be obtained with significantly less computation. We will also see ways of annotating the model for better performance, no matter the inference algorithm.</p>

<h5 id="caveat-the-following-cell-may-run-for-2-3-minutes">Caveat: the following cell may run for 2-3 minutes.</h5>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">traces</span> <span class="o">=</span> <span class="x">[</span><span class="n">do_inference</span><span class="x">(</span><span class="n">changepoint_model</span><span class="x">,</span> <span class="n">xs_dense</span><span class="x">,</span> <span class="n">ys_complex</span><span class="x">,</span> <span class="mi">100000</span><span class="x">)</span> <span class="k">for</span> <span class="n">_</span><span class="o">=</span><span class="mi">1</span><span class="x">:</span><span class="mi">12</span><span class="x">];</span>
<span class="n">grid</span><span class="x">(</span><span class="n">render_changepoint_model_trace</span><span class="x">,</span> <span class="n">traces</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="output_156_0.svg" alt="svg" /></p>

<p>The results show that more segments are inferred for the more complex data set.</p>

<hr />
<h3 id="exercise-6">Exercise</h3>
<p>Write a function that takes a data set of x- and y-coordinates and plots the histogram of the probability distribution on the number of changepoints.
Show the results for the <code class="highlighter-rouge">ys_simple</code> and <code class="highlighter-rouge">ys_complex</code> data sets.</p>

<p>Hint: The return value of <code class="highlighter-rouge">changepoint_model</code> is the tree of <code class="highlighter-rouge">Node</code> values. Walk this tree.</p>

<hr />

<h3 id="exercise-7">Exercise</h3>
<p>Write a new version of <code class="highlighter-rouge">changepoint_model</code> that uses <code class="highlighter-rouge">{*} ~ ...</code> without an address to make the recursive calls.</p>

<p>Hint: You will need to guarantee that all addresses are unique. How can you label each node in a binary tree using an integer?</p>

<hr />


</div>

</main><!-- /.container -->

<!-- Footer -->
<footer class="page-footer font-small blue pt-4">

  <!-- Copyright -->
  <div class="footer-copyright text-center py-3">© 2020 Copyright: The author(s).
  </div>
  <!-- Copyright -->

</footer>
<!-- Footer -->

<script
			  src="https://code.jquery.com/jquery-3.5.1.min.js"
			  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
			  crossorigin="anonymous"></script>
    <!--<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>-->
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha384-B4gt1jrGC7Jh4AgTPSdUtOBvfO8shuf57BaghqFfPlYxofvL8/KUEfYiJOMMV+rV" crossorigin="anonymous"></script>
  </body>
</html>
